# Standalone vs Distributed approach in PySpark and PyTorch
This repository contains the source code and results for a project focused on the performance analysis of distributed deep learning on a multi-core CPU environment. The goal is to compare the training time of a Recurrent Neural Network (LSTM) in a standalone PyTorch setup versus a distributed setup with PySpark's TorchDistributor.

Here you can find the [dataset](https://www.kaggle.com/datasets/andrewmvd/sp-500-stocks)
